---
version: "3"

x-task-vars: &task-vars
  APP: '{{.APP}}'
  CONTROLLER: '{{.CONTROLLER}}'
  NAMESPACE: '{{.NAMESPACE}}'
  CLAIM: '{{.CLAIM}}'
  TS: '{{.TS}}'
  KUSTOMIZATION: '{{.KUSTOMIZATION}}'

vars:
  REPLICATION_DESTINATION_TEMPLATE: "{{.PROJECT_DIR}}/.taskfiles/VolSync/ReplicationDestination.tmpl.yaml"
  WIPE_JOB_TEMPLATE: "{{.PROJECT_DIR}}/.taskfiles/VolSync/WipeJob.tmpl.yaml"
  WAIT_FOR_JOB_SCRIPT: "{{.PROJECT_DIR}}/.taskfiles/VolSync/wait-for-job.sh"
  TS: '{{now | date "150405"}}'

tasks:

  # To run backup jobs in parallel for all replicationsources:
  #  - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=4 -l bash -c 'task volsync:backup APP=$0 NAMESPACE=$1'
  #
  backup:
    desc: Backup a PVC now (ex. task volsync:backup APP=plex [NAMESPACE=default])
    cmds:
      - kubectl -n {{.NAMESPACE}} patch replicationsources {{.APP}} --type merge -p '{"spec":{"trigger":{"manual":"'{{.TS}}'"}}}'
      - bash {{.WAIT_FOR_JOB_SCRIPT}} volsync-src-{{.APP}} {{.NAMESPACE}}
      - kubectl -n {{.NAMESPACE}} wait job/volsync-src-{{.APP}} --for condition=complete --timeout=120m
    vars:
      NAMESPACE: '{{.NAMESPACE | default "default"}}'
    preconditions:
      - sh: test -f {{.WAIT_FOR_JOB_SCRIPT}}

  # To run restore jobs in parallel for all replicationdestinations:
  #   - kubectl get replicationsources --all-namespaces --no-headers | awk '{print $2, $1}' | xargs --max-procs=4 -l bash -c 'task volsync:restore APP=$0 NAMESPACE=$1'
  #
  restore:
    desc: Restore a PVC now (ex. task volsync:restore APP=plex [NAMESPACE=default])
    cmds:
      - task: restore-suspend-app
        vars: *task-vars
      - task: restore-wipe-job
        vars: *task-vars
      - task: restore-volsync-job
        vars: *task-vars
      - task: restore-resume-app
        vars: *task-vars
    vars:
      APP: '{{ or .APP (fail "Variable `APP` is required") }}'
      NAMESPACE: '{{.NAMESPACE | default "default"}}'
      CONTROLLER:
        sh: |
          query="$(kubectl -n {{.NAMESPACE}} get deployment,statefulset --selector="app.kubernetes.io/name={{.APP}}" --no-headers 2>&1)"
          if echo "${query}" | grep -q "No resources"; then
              echo "Controller not found in cluster" && exit 1
          else
              echo "${query}" | awk '{print $1}'
          fi
      CLAIM:
        sh: |
          query="$(kubectl -n {{.NAMESPACE}} get persistentvolumeclaim --selector="app.kubernetes.io/name={{.APP}}" --no-headers 2>&1)"
          if echo "${query}" | grep -q "No resources"; then
              echo "Claim not found in cluster"
          else
              echo "${query}" | awk '{print $1}'
          fi
      KUSTOMIZATION:
        sh: |
          query="$(kubectl -n {{.NAMESPACE}} get helmrelease {{.APP}} -o yaml 2>&1)"
          if echo "${query}" | grep -q "NotFound"; then
              echo "Kustomization not found in cluster" && exit 1
          else
              echo "${query}" | yq eval '.metadata.labels."kustomize.toolkit.fluxcd.io/name"'
          fi
    env: *task-vars
    preconditions:
      - sh: test -f {{.WIPE_JOB_TEMPLATE}}
      - sh: test -f {{.REPLICATION_DESTINATION_TEMPLATE}}
      - sh: test -f {{.WAIT_FOR_JOB_SCRIPT}}

  # Suspend the Flux ks and hr
  restore-suspend-app:
    internal: true
    cmds:
      - flux -n flux-system suspend kustomization {{.KUSTOMIZATION}}
      - flux -n {{.NAMESPACE}} suspend helmrelease {{.APP}}
      - kubectl -n {{.NAMESPACE}} scale {{.CONTROLLER}} --replicas 0
      - kubectl -n {{.NAMESPACE}} wait pod --for delete --selector="app.kubernetes.io/name={{.APP}}" --timeout=2m
    env: *task-vars

  # Wipe the PVC of all data
  restore-wipe-job:
    internal: true
    cmds:
      - envsubst < <(cat {{.WIPE_JOB_TEMPLATE}}) | kubectl apply -f -
      - bash {{.WAIT_FOR_JOB_SCRIPT}} wipe-{{.APP}}-{{.CLAIM}}-{{.TS}} {{.NAMESPACE}}
      - kubectl -n {{.NAMESPACE}} wait job/wipe-{{.APP}}-{{.CLAIM}}-{{.TS}} --for condition=complete --timeout=120m
      - kubectl -n {{.NAMESPACE}} logs job/wipe-{{.APP}}-{{.CLAIM}}-{{.TS}} --container wipe
      - kubectl -n {{.NAMESPACE}} delete job wipe-{{.APP}}-{{.CLAIM}}-{{.TS}}
    env: *task-vars

  # Create VolSync replicationdestination CR to restore data
  restore-volsync-job:
    internal: true
    cmds:
      - envsubst < <(cat {{.REPLICATION_DESTINATION_TEMPLATE}}) | kubectl apply -f -
      - bash {{.WAIT_FOR_JOB_SCRIPT}} volsync-dst-{{.APP}}-{{.CLAIM}}-{{.TS}} {{.NAMESPACE}}
      - kubectl -n {{.NAMESPACE}} wait job/volsync-dst-{{.APP}}-{{.CLAIM}}-{{.TS}} --for condition=complete --timeout=120m
      - kubectl -n {{.NAMESPACE}} delete replicationdestination {{.APP}}-{{.CLAIM}}-{{.TS}}
    env: *task-vars

  # Resume Flux ks and hr
  restore-resume-app:
    internal: true
    cmds:
      - flux -n {{.NAMESPACE}} resume helmrelease {{.APP}}
      - flux -n flux-system resume kustomization {{.KUSTOMIZATION}}
    env: *task-vars
