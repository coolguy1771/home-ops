---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: loki
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://grafana.github.io/helm-charts
      chart: loki
      version: 2.9.0
      sourceRef:
        kind: HelmRepository
        name: grafana-charts
        namespace: flux-system
      interval: 5m
  values:
    image:
      repository: ghcr.io/k8s-at-home/loki
      tag: v2.4.2
    ingress:
      enabled: true
      annotations:
        kubernetes.io/ingress.class: nginx
        cert-manager.io/cluster-issuer: letsencrypt-production
        external-dns.alpha.kubernetes.io/target: "ipv4.${SECRET_PUBLIC_DOMAIN}"
        external-dns/is-public: "true"
      hosts:
        - host: "loki.${SECRET_PUBLIC_DOMAIN}"
          paths:
            - /
      tls:
        - hosts:
            - "loki.${SECRET_PUBLIC_DOMAIN}"
          secretName: loki-tls
    serviceMonitor:
      enabled: true
    config:
      limits_config:
        enforce_metric_name: false
        reject_old_samples: true
        reject_old_samples_max_age: 168h
        ingestion_rate_mb: 8
        ingestion_burst_size_mb: 12
      storage_config:
        aws:
          bucketnames: loki
          endpoint: "s3.${SECRET_PRIVATE_DOMAIN}"
          access_key_id: "${SECRET_S3_ACCESS_KEY}"
          secret_access_key: "${SECRET_S3_SECRET_KEY}"
          s3forcepathstyle: true
          insecure: false
        boltdb_shipper:
          active_index_directory: /data/loki/index
          cache_location: /data/loki/index_cache
          resync_interval: 5s
          shared_store: s3
      ruler:
        storage:
          type: local
          local:
            directory: /rules
        rule_path: /tmp/scratch
        alertmanager_url: http://prometheus-alertmanager:9093
        ring:
          kvstore:
            store: inmemory
        enable_api: true
    alerting_groups:
      #
      # SMART Failures
      #
      - name: smart-failure
        rules:
          - alert: SmartFailures
            expr: |
              sum by (hostname) (count_over_time({hostname=~".+"} | json | _SYSTEMD_UNIT = "smartmontools.service" !~ "(?i)previous self-test completed without error" !~ "(?i)Prefailure" |~ "(?i)(error|fail)"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "SMART has reported failures on host {{ $labels.hostname }}"
      #
      # *arr applications
      #
      - name: arr
        rules:
          - alert: ArrDatabaseIsLocked
            expr: |
              sum by (app) (count_over_time({app=~".*arr"} |~ "(?i)database is locked"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "{{ $labels.app }} is experiencing locked database issues"
          - alert: ArrDatabaseIsMalformed
            expr: |
              sum by (app) (count_over_time({app=~".*arr"} |~ "(?i)database disk image is malformed"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "{{ $labels.app }} is experiencing malformed database disk image issues"
      #
      # vaultwarden
      #
      - name: vaultwarden
        rules:
          - alert: VaultwardenUnableToReachPostgresql
            expr: |
              sum by (app) (count_over_time({app="vaultwarden"} |~ "(?i)could not connect to server"[2m])) > 0
            for: 10s
            labels:
              severity: critical
              category: logs
            annotations:
              summary: "Vaultwarden is unable to connect to postgresql"
