---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: grafana-k8s-monitoring
spec:
  interval: 30m
  chart:
    spec:
      chart: k8s-monitoring
      version: 1.6.33
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      strategy: rollback
      retries: 3
  values:
    # cluster:
    #   name: kyak
    # externalServices:
    #   prometheus:
    #     host: http://mimir-gateway.monitoring.svc:80
    #     writeEndpoint: /api/v1/push
    #     queryEndpoint: /prometheus
    #   loki:
    #     host: http://loki-gateway.monitoring.svc
    #   tempo:
    #     host: http://tempo.monitoring.svc:4317
    #     tls:
    #       insecure: true
    # metrics:
    #   enabled: true
    #   alloy:
    #     metricsTuning:
    #       useIntegrationAllowList: true
    #   cost:
    #     enabled: false
    #   kepler:
    #     enabled: true
    #   node-exporter:
    #     enabled: true
    #   beyla:
    #     enabled: true
    #   apiserver:
    #     enabled: true
    #   kubeControllerManager:
    #     enabled: true
    #   kubeScheduler:
    #     enabled: true
    #   autoDiscover:
    #     enabled: true
    # logs:
    #   enabled: true
    #   pod_logs:
    #     enabled: true
    #   cluster_events:
    #     enabled: true
    #   extraConfig: |-
    #     discovery.relabel "logs_alloy" {
    #       targets = discovery.relabel.pod_logs.output
    #       rule {
    #         source_labels	= ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
    #         regex	= "alloy.*"
    #         action = "keep"
    #       }

    #       rule {
    #         source_labels = ["pod"]
    #         target_label  = "instance"
    #       }

    #       rule {
    #         target_label = "job"
    #         replacement  = "integrations/alloy"
    #       }
    #     }

    #     local.file_match "logs_alloy" {
    #       path_targets = discovery.relabel.logs_alloy.output
    #     }

    #     loki.source.file "logs_alloy" {
    #       targets    = local.file_match.logs_alloy.targets
    #       forward_to = [loki.process.logs_alloy.receiver]
    #     }

    #     loki.process "logs_alloy" {
    #       forward_to = [loki.process.logs_service.receiver]

    #       stage.regex {
    #         expression = "(level=(?P<log_level>[\\s]*debug|warn|info|error))"
    #       }

    #       stage.labels {
    #         values = {
    #           level = "log_level",
    #         }
    #       }

    #       stage.match {
    #         selector = "{tmp_container_runtime=\"docker\"}"
    #         stage.docker {}
    #       }

    #       stage.match {
    #         selector = "{tmp_container_runtime=\"containerd\"}"
    #         stage.cri {}
    #       }
    #     }
    # traces:
    #   enabled: true
    # receivers:
    #   grpc:
    #     enabled: true
    #   http:
    #     enabled: true
    #   zipkin:
    #     enabled: true
    #   grafanaCloudMetrics:
    #     enabled: false
    # opencost:
    #   enabled: false
    # kube-state-metrics:
    #   enabled: true
    # prometheus-node-exporter:
    #   enabled: true
    # prometheus-operator-crds:
    #   enabled: true
    # kepler:
    #   enabled: true
    # alloy: {}
    # alloy-events: {}
    # alloy-logs: {}
    # beyla:
    #   enabled: true
    # extraConfig: |-
    #   discovery.relabel "cilium_agent" {
    #     targets = discovery.kubernetes.services.targets
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_k8s_app"]
    #       regex = "cilium"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_endpoint_port_name"]
    #       regex = "metrics"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_k8s_app"]
    #       regex = "(.+)"
    #       replacement = "${1}"
    #       target_label = "k8s_app"
    #     }
    #   }

    #   prometheus.scrape "cilium_agent" {
    #     targets      = discovery.relabel.cilium_agent.output
    #     job_name     = "integrations/cilium-enterprise/cilium-agent"
    #     honor_labels = true
    #     forward_to   = [prometheus.relabel.metrics_service.receiver]
    #   }

    #   // Cilium Operator
    #   discovery.relabel "cilium_operator" {
    #     targets = discovery.kubernetes.services.targets
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_name"]
    #       regex = "cilium-operator"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_io_cilium_app"]
    #       regex = "operator"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_endpoint_port_name"]
    #       regex = "metrics"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_io_cilium_app_app"]
    #       regex = "(.+)"
    #       replacement = "${1}"
    #       target_label = "io_cilium_app"
    #     }
    #   }

    #   prometheus.scrape "cilium_operator" {
    #     targets      = discovery.relabel.cilium_operator.output
    #     job_name     = "integrations/cilium-enterprise/cilium-operator"
    #     honor_labels = true
    #     forward_to   = [prometheus.relabel.metrics_service.receiver]
    #   }

    #   // Hubble Relay
    #   discovery.relabel "hubble_relay" {
    #     targets = discovery.kubernetes.services.targets
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_k8s_app"]
    #       regex = "hubble-relay"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_endpoint_port_name"]
    #       regex = "metrics"
    #       action = "keep"
    #     }
    #   }

    #   prometheus.scrape "hubble_relay" {
    #     targets    = discovery.relabel.hubble_relay.output
    #     job_name   = "integrations/cilium-enterprise/hubble-relay"
    #     forward_to = [prometheus.relabel.metrics_service.receiver]
    #   }

    #   // Hubble
    #   discovery.relabel "hubble" {
    #     targets = discovery.kubernetes.services.targets
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_k8s_app"]
    #       regex = "hubble"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_endpoint_port_name"]
    #       regex = "hubble-metrics"
    #       action = "keep"
    #     }
    #   }

    #   prometheus.scrape "hubble" {
    #     targets      = discovery.relabel.hubble.output
    #     job_name     = "integrations/cilium-enterprise/hubble"
    #     honor_labels = true
    #     forward_to   = [prometheus.relabel.metrics_service.receiver]
    #   }

    #   // Hubble Enterprise
    #   discovery.relabel "hubble_enterprise" {
    #     targets = discovery.kubernetes.services.targets
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
    #       regex = "hubble-enterprise"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_endpoint_port_name"]
    #       regex = "metrics"
    #       action = "keep"
    #     }
    #   }

    #   prometheus.scrape "hubble_enterprise" {
    #     targets      = discovery.relabel.hubble_enterprise.output
    #     job_name     = "integrations/cilium-enterprise/hubble-enterprise"
    #     honor_labels = true
    #     forward_to   = [prometheus.relabel.metrics_service.receiver]
    #   }

    #   // Hubble Timescape Ingester
    #   discovery.relabel "hubble_timescape_ingester" {
    #     targets = discovery.kubernetes.services.targets
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
    #       regex = "hubble-timescape-ingester"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_component"]
    #       regex = "ingester"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_endpoint_port_name"]
    #       regex = "metrics"
    #       action = "keep"
    #     }
    #   }

    #   prometheus.scrape "hubble_timescape_ingester" {
    #     targets      = discovery.relabel.hubble_timescape_ingester.output
    #     job_name     = "integrations/cilium-enterprise/hubble-timescape-ingester"
    #     honor_labels = true
    #     forward_to   = [prometheus.relabel.metrics_service.receiver]
    #   }

    #   // Hubble Timescape Server
    #   discovery.relabel "hubble_timescape_server" {
    #     targets = discovery.kubernetes.services.targets
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_name"]
    #       regex = "hubble-timescape-server"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_service_label_app_kubernetes_io_component"]
    #       regex = "server"
    #       action = "keep"
    #     }
    #     rule {
    #       source_labels = ["__meta_kubernetes_endpoint_port_name"]
    #       regex = "metrics"
    #       action = "keep"
    #     }
    #   }

    #   prometheus.scrape "hubble_timescape_server" {
    #     targets      = discovery.relabel.hubble_timescape_server.output
    #     job_name     = "integrations/cilium-enterprise/hubble-timescape-server"
    #     honor_labels = true
    #     forward_to   = [prometheus.relabel.metrics_service.receiver]
    #   }
    cluster:
      # -- The name for this cluster.
      # @section -- Cluster
      name: "kyak"

    #
    # Destinations
    #

    # -- The list of destinations where telemetry data will be sent.
    # See the [destinations documentation](https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/docs/destinations/README.md) for more information.
    # @section -- Destinations
    destinations:
      - name: mimir
        type: prometheus
        url: http://mimir-gateway.monitoring.svc:80/api/v1/push
      - name: hostedLogs
        type: loki
        url: http://loki-gateway.monitoring.svc/loki/api/v1/push
        auth:
          type: basic
          username: "my-username"
          password: "my-password"
          tenantIdFrom: env("LOKI_TENANT_ID")
      - name: tempo
        type: otlp
        url: http://tempo.monitoring.svc:4317v1/traces
        metrics: { enabled: true }
        logs:    { enabled: true }
        traces:  { enabled: true }
    #   prometheus:
    #     host: http://mimir-gateway.monitoring.svc:80
    #     writeEndpoint: /api/v1/push
    #     queryEndpoint: /prometheus
    #   loki:
    #     host: http://loki-gateway.monitoring.svc
    #   tempo:
    #     host: http://tempo.monitoring.svc:4317
    #     tls:
    #       insecure: true
    #
    # Features
    #

    # -- Cluster Monitoring enables observability and monitoring for your Kubernetes Cluster itself.
    # Requires a destination that supports metrics.
    # To see the valid options, please see the [Cluster Monitoring feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-cluster-metrics).
    # @default -- Disabled
    # @section -- Features - Cluster Metrics
    clusterMetrics:
      # -- Enable gathering Kubernetes Cluster metrics.
      # @section -- Features - Cluster Metrics
      enabled: true

      # -- The destinations where cluster metrics will be sent. If empty, all metrics-capable destinations will be used.
      # @section -- Features - Cluster Metrics
      destinations: []

      # -- Which collector to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Cluster Metrics
      # @ignored
      collector: alloy-metrics

    # -- Cluster events.
    # Requires a destination that supports logs.
    # To see the valid options, please see the [Cluster Events feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-cluster-events).
    # @default -- Disabled
    # @section -- Features - Cluster Events
    clusterEvents:
      # -- Enable gathering Kubernetes Cluster events.
      # @section -- Features - Cluster Events
      enabled: true

      # -- The destinations where cluster events will be sent. If empty, all logs-capable destinations will be used.
      # @section -- Features - Cluster Events
      destinations: []

      # -- Which collector to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Cluster Events
      # @ignored
      collector: alloy-singleton

    # -- Node logs.
    # Requires a destination that supports logs.
    # To see the valid options, please see the [Node Logs feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-node-logs).
    # @default -- Disabled
    # @section -- Features - Node Logs
    nodeLogs:
      # -- Enable gathering Kubernetes Cluster Node logs.
      # @section -- Features - Node Logs
      enabled: true

      # -- The destinations where logs will be sent. If empty, all logs-capable destinations will be used.
      # @section -- Features - Node Logs
      destinations: []

      collector: alloy-logs

    # -- Pod logs.
    # Requires a destination that supports logs.
    # To see the valid options, please see the [Pod Logs feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-pod-logs).
    # @default -- Disabled
    # @section -- Features - Pod Logs
    podLogs:
      # -- Enable gathering Kubernetes Pod logs.
      # @section -- Features - Pod Logs
      enabled: true

      # -- The destinations where logs will be sent. If empty, all logs-capable destinations will be used.
      # @section -- Features - Pod Logs
      destinations: []

      collector: alloy-logs

    # -- Application Observability.
    # Requires destinations that supports metrics, logs, and traces.
    # To see the valid options, please see the [Application Observability feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-application-observability).
    # @default -- Disabled
    # @section -- Features - Application Observability
    applicationObservability:
      # -- Enable gathering Kubernetes Pod logs.
      # @section -- Features - Application Observability
      enabled: true

      # -- The destinations where application data will be sent. If empty, all capable destinations will be used.
      # @section -- Features - Application Observability
      destinations: []

      # -- Which collector to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Application Observability
      # @ignored
      collector: alloy-receiver

    # -- Auto-Instrumentation.
    # Requires destinations that supports metrics, logs, and traces.
    # To see the valid options, please see the [Application Observability feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-application-observability).
    # @default -- Disabled
    # @section -- Features - Auto-Instrumentation
    autoInstrumentation:
      # -- Enable automatic instrumentation for applications.
      # @section -- Features - Auto-Instrumentation
      enabled: true

      # -- The destinations where application data will be sent. If empty, all capable destinations will be used.
      # @section -- Features - Auto-Instrumentation
      destinations: []

      # -- Which collector to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Auto-Instrumentation
      # @ignored
      collector: alloy-metrics

    # -- Annotation Autodiscovery enables gathering metrics from Kubernetes Pods and Services discovered by special annotations.
    # Requires a destination that supports metrics.
    # To see the valid options, please see the [Annotation Autodiscovery feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-annotation-autodiscovery).
    # @default -- Disabled
    # @section -- Features - Annotation Autodiscovery
    annotationAutodiscovery:
      # -- Enable gathering metrics from Kubernetes Pods and Services discovered by special annotations.
      # @section -- Features - Annotation Autodiscovery
      enabled: true

      # -- The destinations where cluster metrics will be sent. If empty, all metrics-capable destinations will be used.
      # @section -- Features - Annotation Autodiscovery
      destinations: []

      # -- Which collector to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Annotation Autodiscovery
      # @ignored
      collector: alloy-metrics

    # -- Prometheus Operator Objects enables the gathering of metrics from objects like Probes, PodMonitors, and
    # ServiceMonitors. Requires a destination that supports metrics.
    # To see the valid options, please see the
    # [Prometheus Operator Objects feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-prometheus-operator-objects).
    # @default -- Disabled
    # @section -- Features - Prometheus Operator Objects
    prometheusOperatorObjects:
      # -- Enable gathering metrics from Prometheus Operator Objects.
      # @section -- Features - Prometheus Operator Objects
      enabled: true

      # -- The destinations where metrics will be sent. If empty, all metrics-capable destinations will be used.
      # @section -- Features - Prometheus Operator Objects
      destinations: []

      # -- Which collector to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Prometheus Operator Objects
      # @ignored
      collector: alloy-metrics

    # -- Profiling enables gathering profiles from applications.
    # Requires a destination that supports profiles.
    # To see the valid options, please see the [Profiling feature documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-profiling).
    # @default -- Disabled
    # @section -- Features - Profiling
    profiling:
      # -- Enable gathering profiles from applications.
      # @section -- Features - Profiling
      enabled: true

      # -- The destinations where profiles will be sent. If empty, all profiles-capable destinations will be used.
      # @section -- Features - Profiling
      destinations: []

      # -- Which collector to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Profiling
      # @ignored
      collector: alloy-profiles

    # -- Service Integrations enables gathering telemetry data for common services and applications deployed to Kubernetes.
    # To see the valid options, please see the [Service Integrations documentation](https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/feature-integrations).
    # @default -- No integrations enabled
    # @section -- Features - Service Integrations
    integrations:
      # -- The destinations where integration metrics will be sent. If empty, all metrics-capable destinations will be used.
      # @section -- Features - Service Integrations
      destinations: []

      # -- Which collectors to assign this feature to. Do not change this unless you are sure of what you are doing.
      # @section -- Features - Service Integrations
      # @ignored
      collector: alloy-metrics

    # -- Self-reporting creates a single metric and log that reports anonymized information about how this Helm chart was
    # configured. It reports features enabled, destinations types used, and alloy instances enabled. It does not report any
    # actual telemetry data, credentials or configuration, or send any data to any destination other than the ones
    # configured above.
    # @section -- Features - Self-reporting
    selfReporting:
      # -- Enable Self-reporting.
      # @section -- Features - Self-reporting
      enabled: false

      # -- How frequently to generate self-report metrics. This does utilize the global scrapeInterval setting.
      # @section -- Features - Self-reporting
      scrapeInterval: 1h

    #
    # Collectors (Alloy instances)
    #

    # An Alloy instance for collecting metrics.
    alloy-metrics:
      # -- Deploy the Alloy instance for collecting metrics.
      # @section -- Collectors - Alloy Metrics
      enabled: true

      # -- Extra Alloy configuration to be added to the configuration file.
      # @section -- Collectors - Alloy Metrics
      extraConfig: ""

      # Remote configuration from a remote config server.
      remoteConfig:
        # -- Enable fetching configuration from a remote config server.
        # @section -- Collectors - Alloy Metrics
        enabled: false

        # -- The URL of the remote config server.
        # @section -- Collectors - Alloy Metrics
        url: ""

        auth:
          # -- The type of authentication to use for the remote config server.
          # @section -- Collectors - Alloy Metrics
          type: "none"

          # -- The username to use for the remote config server.
          # @section -- Collectors - Alloy Metrics
          username: ""
          # -- The key for storing the username in the secret.
          # @section -- Collectors - Alloy Metrics
          usernameKey: "username"
          # -- Raw config for accessing the password.
          # @section -- Collectors - Alloy Metrics
          usernameFrom: ""

          # -- The password to use for the remote config server.
          # @section -- Collectors - Alloy Metrics
          password: ""
          # -- The key for storing the password in the secret.
          # @section -- Collectors - Alloy Metrics
          passwordKey: "password"
          # -- Raw config for accessing the password.
          # @section -- Collectors - Alloy Metrics
          passwordFrom: ""

        secret:
          # -- Whether to create a secret for the remote config server.
          # @section -- Collectors - Alloy Metrics
          create: true
          # -- If true, skip secret creation and embed the credentials directly into the configuration.
          # @section -- Collectors - Alloy Metrics
          embed: false
          # -- The name of the secret to create.
          # @section -- Collectors - Alloy Metrics
          name: ""
          # -- The namespace for the secret.
          # @section -- Collectors - Alloy Metrics
          namespace: ""

        # -- (string) The unique identifier for this Alloy instance.
        # @default -- `<cluster>-<namespace>-<pod-name>`
        # @section -- Collectors - Alloy Metrics
        id: ""

        # -- The frequency at which to poll the remote config server for updates.
        # @section -- Collectors - Alloy Metrics
        pollFrequency: 5m

        # -- Attributes to be added to this collector when requesting configuration.
        # @section -- Collectors - Alloy Metrics
        extraAttributes: {}

      logging:
        # -- Level at which Alloy log lines should be written.
        # @section -- Collectors - Alloy Metrics
        level: info
        # -- Format to use for writing Alloy log lines.
        # @section -- Collectors - Alloy Metrics
        format: logfmt

      liveDebugging:
        # -- Enable live debugging for the Alloy instance.
        # Requires stability level to be set to "experimental".
        # @section -- Collectors - Alloy Metrics
        enabled: false

      # @ignored
      alloy:
        configMap: {create: false}

        # Enable clustering to ensure that scraping is distributed across all instances.
        # @ignored
        clustering:
          name: alloy-metrics
          enabled: true

        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
            add: ["CHOWN", "DAC_OVERRIDE", "FOWNER", "FSETID", "KILL", "SETGID", "SETUID", "SETPCAP", "NET_BIND_SERVICE", "NET_RAW", "SYS_CHROOT", "MKNOD", "AUDIT_WRITE", "SETFCAP"]
          seccompProfile:
            type: "RuntimeDefault"

      controller:
        # -- The type of controller to use for the Alloy Metrics instance.
        # @section -- Collectors - Alloy Metrics
        type: statefulset

        # -- The number of replicas for the Alloy Metrics instance.
        # @section -- Collectors - Alloy Metrics
        replicas: 1

        # @ignored
        nodeSelector:
          kubernetes.io/os: linux

        # @ignored
        podAnnotations:
          k8s.grafana.com/logs.job: integrations/alloy

      # Skip installation of the Grafana Alloy CRDs, since we don't use them in this chart
      # @ignored
      crds: {create: false}

    # An Alloy instance for data sources required to be deployed on a single replica.
    alloy-singleton:
      # -- Deploy the Alloy instance for data sources required to be deployed on a single replica.
      # @section -- Collectors - Alloy Singleton
      enabled: true

      # -- Extra Alloy configuration to be added to the configuration file.
      # @section -- Collectors - Alloy Singleton
      extraConfig: ""

      # Remote configuration from a remote config server.
      remoteConfig:
        # -- Enable fetching configuration from a remote config server.
        # @section -- Collectors - Alloy Singleton
        enabled: false

        # -- The URL of the remote config server.
        # @section -- Collectors - Alloy Singleton
        url: ""

        auth:
          # -- The type of authentication to use for the remote config server.
          # @section -- Collectors - Alloy Singleton
          type: "none"

          # -- The username to use for the remote config server.
          # @section -- Collectors - Alloy Singleton
          username: ""
          # -- The key for storing the username in the secret.
          # @section -- Collectors - Alloy Singleton
          usernameKey: "username"
          # -- Raw config for accessing the username.
          # @section -- Collectors - Alloy Singleton
          usernameFrom: ""

          # -- The password to use for the remote config server.
          # @section -- Collectors - Alloy Singleton
          password: ""
          # -- The key for storing the password in the secret.
          # @section -- Collectors - Alloy Singleton
          passwordKey: "password"
          # -- Raw config for accessing the password.
          # @section -- Collectors - Alloy Singleton
          passwordFrom: ""

        secret:
          # -- Whether to create a secret for the remote config server.
          # @section -- Collectors - Alloy Singleton
          create: true
          # -- If true, skip secret creation and embed the credentials directly into the configuration.
          # @section -- Collectors - Alloy Singleton
          embed: false
          # -- The name of the secret to create.
          # @section -- Collectors - Alloy Singleton
          name: ""
          # -- The namespace for the secret.
          # @section -- Collectors - Alloy Singleton
          namespace: ""

      logging:
        # -- Level at which Alloy log lines should be written.
        # @section -- Collectors - Alloy Singleton
        level: info
        # -- Format to use for writing Alloy log lines.
        # @section -- Collectors - Alloy Singleton
        format: logfmt

      liveDebugging:
        # -- Enable live debugging for the Alloy instance.
        # Requires stability level to be set to "experimental".
        # @section -- Collectors - Alloy Singleton
        enabled: false

      # @ignored
      alloy:
        # This chart is creating the configuration, so the alloy chart does not need to.
        configMap: {create: false}

        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
            add: ["CHOWN", "DAC_OVERRIDE", "FOWNER", "FSETID", "KILL", "SETGID", "SETUID", "SETPCAP", "NET_BIND_SERVICE", "NET_RAW", "SYS_CHROOT", "MKNOD", "AUDIT_WRITE", "SETFCAP"]
          seccompProfile:
            type: "RuntimeDefault"

      controller:
        # -- The type of controller to use for the Alloy Singleton instance.
        # @section -- Collectors - Alloy Singleton
        type: deployment
        # -- The number of replicas for the Alloy Singleton instance.
        # This should remain a single instance to avoid duplicate data.
        # @section -- Collectors - Alloy Singleton
        replicas: 1

        # @ignored
        nodeSelector:
          kubernetes.io/os: linux

        # @ignored
        podAnnotations:
          k8s.grafana.com/logs.job: integrations/alloy

      # Skip installation of the Grafana Alloy CRDs, since we don't use them in this chart
      # @ignored
      crds: {create: false}

    # An Alloy instance for collecting log data.
    alloy-logs:
      # -- Deploy the Alloy instance for collecting log data.
      # @section -- Collectors - Alloy Logs
      enabled: true

      # -- Extra Alloy configuration to be added to the configuration file.
      # @section -- Collectors - Alloy Logs
      extraConfig: ""

      # Remote configuration from a remote config server.
      remoteConfig:
        # -- Enable fetching configuration from a remote config server.
        # @section -- Collectors - Alloy Logs
        enabled: false

        # -- The URL of the remote config server.
        # @section -- Collectors - Alloy Logs
        url: ""

        auth:
          # -- The type of authentication to use for the remote config server.
          # @section -- Collectors - Alloy Logs
          type: "none"

          # -- The username to use for the remote config server.
          # @section -- Collectors - Alloy Logs
          username: ""
          # -- The key for storing the username in the secret.
          # @section -- Collectors - Alloy Logs
          usernameKey: "username"
          # -- Raw config for accessing the username.
          # @section -- Collectors - Alloy Logs
          usernameFrom: ""

          # -- The password to use for the remote config server.
          # @section -- Collectors - Alloy Logs
          password: ""
          # -- The key for storing the username in the secret.
          # @section -- Collectors - Alloy Logs
          passwordKey: "password"
          # -- Raw config for accessing the password.
          # @section -- Collectors - Alloy Logs
          passwordFrom: ""

        secret:
          # -- Whether to create a secret for the remote config server.
          # @section -- Collectors - Alloy Logs
          create: true
          # -- If true, skip secret creation and embed the credentials directly into the configuration.
          # @section -- Collectors - Alloy Logs
          embed: false
          # -- The name of the secret to create.
          # @section -- Collectors - Alloy Logs
          name: ""
          # -- The namespace for the secret.
          # @section -- Collectors - Alloy Logs
          namespace: ""

      logging:
        # -- Level at which Alloy log lines should be written.
        # @section -- Collectors - Alloy Logs
        level: info
        # -- Format to use for writing Alloy log lines.
        # @section -- Collectors - Alloy Logs
        format: logfmt

      liveDebugging:
        # -- Enable live debugging for the Alloy instance.
        # Requires stability level to be set to "experimental".
        # @section -- Collectors - Alloy Logs
        enabled: false

      # @ignored
      alloy:
        # This chart is creating the configuration, so the alloy chart does not need to.
        configMap: {create: false}

        # Disabling clustering by default, because the default log gathering format does not require clusters.
        clustering: {enabled: false}

        # @ignored
        mounts:
          # Mount /var/log from the host into the container for log collection.
          varlog: true
          # Mount /var/lib/docker/containers from the host into the container for log
          # collection. Set to true if your cluster puts log files inside this directory.
          dockercontainers: true

        # @ignored
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
            add: ["CHOWN", "DAC_OVERRIDE", "FOWNER", "FSETID", "KILL", "SETGID", "SETUID", "SETPCAP", "NET_BIND_SERVICE", "NET_RAW", "SYS_CHROOT", "MKNOD", "AUDIT_WRITE", "SETFCAP"]
          seccompProfile:
            type: "RuntimeDefault"

      controller:
        # -- The type of controller to use for the Alloy Logs instance.
        # @section -- Collectors - Alloy Logs
        type: daemonset

        # @ignored
        nodeSelector:
          kubernetes.io/os: linux

      # Skip installation of the Grafana Alloy CRDs, since we don't use them in this chart
      # @ignored
      crds: {create: false}

    # An Alloy instance for opening receivers to collect application data.
    alloy-receiver:
      # -- Deploy the Alloy instance for opening receivers to collect application data.
      # @section -- Collectors - Alloy Receiver
      enabled: false

      # -- Extra Alloy configuration to be added to the configuration file.
      # @section -- Collectors - Alloy Receiver
      extraConfig: ""

      # Remote configuration from a remote config server.
      remoteConfig:
        # -- Enable fetching configuration from a remote config server.
        # @section -- Collectors - Alloy Receiver
        enabled: false

        # -- The URL of the remote config server.
        # @section -- Collectors - Alloy Receiver
        url: ""

        auth:
          # -- The type of authentication to use for the remote config server.
          # @section -- Collectors - Alloy Receiver
          type: "none"

          # -- The username to use for the remote config server.
          # @section -- Collectors - Alloy Receiver
          username: ""
          # -- The key for storing the username in the secret.
          # @section -- Collectors - Alloy Receiver
          usernameKey: "username"
          # -- Raw config for accessing the username.
          # @section -- Collectors - Alloy Receiver
          usernameFrom: ""

          # -- The password to use for the remote config server.
          # @section -- Collectors - Alloy Receiver
          password: ""
          # -- The key for storing the password in the secret.
          # @section -- Collectors - Alloy Receiver
          passwordKey: "password"
          # -- Raw config for accessing the password.
          # @section -- Collectors - Alloy Receiver
          passwordFrom: ""

        secret:
          # -- Whether to create a secret for the remote config server.
          # @section -- Collectors - Alloy Receiver
          create: true
          # -- If true, skip secret creation and embed the credentials directly into the configuration.
          # @section -- Collectors - Alloy Receiver
          embed: false
          # -- The name of the secret to create.
          # @section -- Collectors - Alloy Receiver
          name: ""
          # -- The namespace for the secret.
          # @section -- Collectors - Alloy Receiver
          namespace: ""

      logging:
        # -- Level at which Alloy log lines should be written.
        # @section -- Collectors - Alloy Receiver
        level: info
        # -- Format to use for writing Alloy log lines.
        # @section -- Collectors - Alloy Receiver
        format: logfmt

      liveDebugging:
        # -- Enable live debugging for the Alloy instance.
        # Requires stability level to be set to "experimental".
        # @section -- Collectors - Alloy Receiver
        enabled: false

      alloy:
        # -- The ports to expose for the Alloy receiver.
        # @section -- Collectors - Alloy Receiver
        extraPorts: []

        # This chart is creating the configuration, so the alloy chart does not need to.
        # @ignored
        configMap: {create: false}

        # @ignored
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["ALL"]
            add: ["CHOWN", "DAC_OVERRIDE", "FOWNER", "FSETID", "KILL", "SETGID", "SETUID", "SETPCAP", "NET_BIND_SERVICE", "NET_RAW", "SYS_CHROOT", "MKNOD", "AUDIT_WRITE", "SETFCAP"]
          seccompProfile:
            type: "RuntimeDefault"

      controller:
        # -- The type of controller to use for the Alloy Receiver instance.
        # @section -- Collectors - Alloy Receiver
        type: daemonset

        # @ignored
        nodeSelector:
          kubernetes.io/os: linux

      # Skip installation of the Grafana Alloy CRDs, since we don't use them in this chart
      # @ignored
      crds: {create: false}

    # An Alloy instance for gathering profiles.
    alloy-profiles:
      # -- Deploy the Alloy instance for gathering profiles.
      # @section -- Collectors - Alloy Profiles
      enabled: true

      # -- Extra Alloy configuration to be added to the configuration file.
      # @section -- Collectors - Alloy Profiles
      extraConfig: ""

      # Remote configuration from a remote config server.
      remoteConfig:
        # -- Enable fetching configuration from a remote config server.
        # @section -- Collectors - Alloy Profiles
        enabled: false

        # -- The URL of the remote config server.
        # @section -- Collectors - Alloy Profiles
        url: ""

        auth:
          # -- The type of authentication to use for the remote config server.
          # @section -- Collectors - Alloy Profiles
          type: "none"

          # -- The username to use for the remote config server.
          # @section -- Collectors - Alloy Profiles
          username: ""
          # -- The key for storing the username in the secret.
          # @section -- Collectors - Alloy Profiles
          usernameKey: "username"
          # -- Raw config for accessing the username.
          # @section -- Collectors - Alloy Profiles
          usernameFrom: ""

          # -- The password to use for the remote config server.
          # @section -- Collectors - Alloy Profiles
          password: ""
          # -- The key for storing the password in the secret.
          # @section -- Collectors - Alloy Profiles
          passwordKey: "password"
          # -- Raw config for accessing the password.
          # @section -- Collectors - Alloy Profiles
          passwordFrom: ""

        secret:
          # -- Whether to create a secret for the remote config server.
          # @section -- Collectors - Alloy Profiles
          create: true
          # -- If true, skip secret creation and embed the credentials directly into the configuration.
          # @section -- Collectors - Alloy Profiles
          embed: false
          # -- The name of the secret to create.
          # @section -- Collectors - Alloy Profiles
          name: ""
          # -- The namespace for the secret.
          # @section -- Collectors - Alloy Profiles
          namespace: ""

      logging:
        # -- Level at which Alloy log lines should be written.
        # @section -- Collectors - Alloy Profiles
        level: info
        # -- Format to use for writing Alloy log lines.
        # @section -- Collectors - Alloy Profiles
        format: logfmt

      liveDebugging:
        # -- Enable live debugging for the Alloy instance.
        # Requires stability level to be set to "experimental".
        # @section -- Collectors - Alloy Profiles
        enabled: false

      # @ignored
      alloy:
        # Pyroscope components are currently in public preview
        stabilityLevel: public-preview

        # This chart is creating the configuration, so the alloy chart does not need to.
        configMap: {create: false}

        # Disabling clustering because each instance will gather profiles for the workloads on the same node.
        clustering:
          name: alloy-profiles
          enabled: false

        securityContext:
          privileged: true
          runAsGroup: 0
          runAsUser: 0

      controller:
        # -- The type of controller to use for the Alloy Profiles instance.
        # @section -- Collectors - Alloy Profiles
        type: daemonset

        # @ignored
        hostPID: true

        # @ignored
        nodeSelector:
          kubernetes.io/os: linux

        # @ignored
        tolerations:
          - effect: NoSchedule
            operator: Exists

      # Skip installation of the Grafana Alloy CRDs, since we don't use them in this chart
      # @ignored
      crds: {create: false}

