---
# yaml-language-server: $schema=https://kubernetes-schemas.devbu.io/helm.toolkit.fluxcd.io/helmrelease_v2beta1.json
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: tempo
  namespace: monitoring
spec:
  interval: 15m
  chart:
    spec:
      chart: tempo-distributed
      version: 1.2.7
      sourceRef:
        kind: HelmRepository
        name: grafana
        namespace: flux-system
  maxHistory: 3
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    # -- If true, Tempo will report anonymous usage data about the shape of a deployment to Grafana Labs
    reportingEnabled: false
    tempo:
      # -- Node selector for ingester pods
      nodeSelector: {}
      # -- Tolerations for ingester pods
      tolerations: []
      # -- Extra volumes for ingester pods
      extraVolumeMounts: []
      # -- Extra volumes for ingester deployment
      extraVolumes: []
      persistence:
        # -- Enable creating PVCs which is required when using boltdb-shipper
        enabled: false
        # -- use emptyDir with ramdisk instead of PVC. **Please note that all data in ingester will be lost on pod restart**
        inMemory: false
        # -- Size of persistent or memory disk
        size: 10Gi
        # -- Storage class to be used.
        # If defined, storageClassName: <storageClass>.
        # If set to "-", storageClassName: "", which disables dynamic provisioning.
        # If empty or set to null, no storageClassName spec is
        # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
        storageClass: null
        # -- Annotations for ingester's persist volume claim
        annotations: {}
      config:
        # -- Number of copies of spans to store in the ingester ring
        replication_factor: 3
        # -- Amount of time a trace must be idle before flushing it to the wal.
        trace_idle_period: null
        # -- How often to sweep all tenants and move traces from live -> wal -> completed blocks.
        flush_check_period: null
        # -- Maximum size of a block before cutting it
        max_block_bytes: null
        # -- Maximum length of time before cutting a block
        max_block_duration: null
        # -- Duration to keep blocks in the ingester after they have been flushed
        complete_block_timeout: null
      service:
        # -- Annotations for ingester service
        annotations: {}
      # -- Adds the appProtocol field to the ingester service. This allows ingester to work with istio protocol selection.
      appProtocol:
        # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
        grpc: null

    # Configuration for the metrics-generator
    metricsGenerator:
      # -- Specifies whether a metrics-generator should be deployed
      enabled: false
      # -- Annotations for the metrics-generator StatefulSet
      annotations: {}
      # -- Number of replicas for the metrics-generator
      replicas: 1
      image:
        # -- The Docker registry for the metrics-generator image. Overrides `tempo.image.registry`
        registry: null
        # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
        pullSecrets: []
        # -- Docker image repository for the metrics-generator image. Overrides `tempo.image.repository`
        repository: null
        # -- Docker image tag for the metrics-generator image. Overrides `tempo.image.tag`
        tag: null
      # -- The name of the PriorityClass for metrics-generator pods
      priorityClassName: null
      # -- Labels for metrics-generator pods
      podLabels: {}
      # -- Annotations for metrics-generator pods
      podAnnotations: {}
      # -- Additional CLI args for the metrics-generator
      extraArgs: []
      # -- Environment variables to add to the metrics-generator pods
      extraEnv: []
      # -- Environment variables from secrets or configmaps to add to the metrics-generator pods
      extraEnvFrom: []
      # -- Resource requests and limits for the metrics-generator
      resources: {}
      # -- Grace period to allow the metrics-generator to shutdown before it is killed. Especially for the ingestor,
      # this must be increased. It must be long enough so metrics-generators can be gracefully shutdown flushing/transferring
      # all data and to successfully leave the member ring on shutdown.
      terminationGracePeriodSeconds: 300
      # -- topologySpread for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Defaults to allow skew no more then 1 node per AZ
      topologySpreadConstraints: |
        - maxSkew: 1
          topologyKey: failure-domain.beta.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 6 }}
      # -- Affinity for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Hard node and soft zone anti-affinity
      affinity: |
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 10 }}
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 12 }}
                topologyKey: failure-domain.beta.kubernetes.io/zone
      # -- Node selector for metrics-generator pods
      nodeSelector: {}
      # -- Tolerations for metrics-generator pods
      tolerations: []
      # -- The EmptyDir location where the /var/tempo will be mounted on. Defaults to local disk, can be set to memory.
      walEmptyDir: {}
        ## Here shows how to configure 1Gi memory as emptyDir.
        ## Ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#emptydirvolumesource-v1-core
        # medium: "Memory"
        # sizeLimit: 1Gi
      # -- Extra volumes for metrics-generator pods
      extraVolumeMounts: []
      # -- Extra volumes for metrics-generator deployment
      extraVolumes: []
      # -- Default ports
      ports:
        - name: grpc
          port: 9095
          service: true
        - name: http-memberlist
          port: 7946
          service: false
        - name: http-metrics
          port: 3100
          service: true
      # -- More information on configuration: https://grafana.com/docs/tempo/latest/configuration/#metrics-generator
      config:
        registry:
          collection_interval: 15s
          external_labels: {}
          stale_duration: 15m
        processor:
          service_graphs:
            # -- Additional dimensions to add to the metrics. Dimensions are searched for in the
            # -- resource and span attributes and are added to the metrics if present.
            dimensions: []
            histogram_buckets: [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8]
            max_items: 10000
            wait: 10s
            workers: 10
          span_metrics:
            # -- Additional dimensions to add to the metrics along with the default dimensions.
            # -- Dimensions are searched for in the resource and span attributes and are added to the metrics if present.
            dimensions: []
            histogram_buckets: [0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128, 0.256, 0.512, 1.02, 2.05, 4.10]
        storage:
          path: /var/tempo/wal
          wal:
          remote_write_flush_deadline: 1m
          # -- A list of remote write endpoints.
          # -- https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
          remote_write: []
      service:
        # -- Annotations for Metrics Generator service
        annotations: {}
      # -- Adds the appProtocol field to the metricsGenerator service. This allows metricsGenerator to work with istio protocol selection.
      appProtocol:
        # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
        grpc: null

    # Configuration for the distributor
    distributor:
      # -- Number of replicas for the distributor
      replicas: 1
      autoscaling:
        # -- Enable autoscaling for the distributor
        enabled: false
        # -- Minimum autoscaling replicas for the distributor
        minReplicas: 1
        # -- Maximum autoscaling replicas for the distributor
        maxReplicas: 3
        # -- Target CPU utilisation percentage for the distributor
        targetCPUUtilizationPercentage: 60
        # -- Target memory utilisation percentage for the distributor
        targetMemoryUtilizationPercentage:
      image:
        # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`
        registry: null
        # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
        pullSecrets: []
        # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`
        repository: null
        # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`
        tag: null
      service:
        # -- Annotations for distributor service
        annotations: {}
        # -- Type of service for the distributor
        type: ClusterIP
        # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
        loadBalancerIP: ''
        # -- If type is LoadBalancer limit incoming traffic from IPs.
        loadBalancerSourceRanges: []
      # -- The name of the PriorityClass for distributor pods
      priorityClassName: null
      # -- Labels for distributor pods
      podLabels: {}
      # -- Annotations for distributor pods
      podAnnotations: {}
      # -- Additional CLI args for the distributor
      extraArgs: []
      # -- Environment variables to add to the distributor pods
      extraEnv: []
      # -- Environment variables from secrets or configmaps to add to the distributor pods
      extraEnvFrom: []
      # -- Resource requests and limits for the distributor
      resources: {}
      # -- Grace period to allow the distributor to shutdown before it is killed
      terminationGracePeriodSeconds: 30
      # -- topologySpread for distributor pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Defaults to allow skew no more then 1 node per AZ
      topologySpreadConstraints: |
        - maxSkew: 1
          topologyKey: failure-domain.beta.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 6 }}
      # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Hard node and soft zone anti-affinity
      affinity: |
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 10 }}
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 12 }}
                topologyKey: failure-domain.beta.kubernetes.io/zone
      # -- Node selector for distributor pods
      nodeSelector: {}
      # -- Tolerations for distributor pods
      tolerations: []
      # -- Extra volumes for distributor pods
      extraVolumeMounts: []
      # -- Extra volumes for distributor deployment
      extraVolumes: []
      config:
        # -- Enable to log every received trace id to help debug ingestion
        # -- WARNING: Deprecated. Use log_received_spans instead.
        log_received_traces: null
        # -- Enable to log every received span to help debug ingestion or calculate span error distributions using the logs
        log_received_spans:
          enabled: false
          include_all_attributes: false
          filter_by_status_error: false
        # -- Disables write extension with inactive ingesters
        extend_writes: null
        # -- List of tags that will not be extracted from trace data for search lookups
        search_tags_deny_list: []
      # -- Adds the appProtocol field to the distributor service. This allows distributor to work with istio protocol selection.
      appProtocol:
        # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
        grpc: null

    compactor:
      # -- Number of replicas for the compactor
      replicas: 1
      image:
        # -- The Docker registry for the compactor image. Overrides `tempo.image.registry`
        registry: null
        # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
        pullSecrets: []
        # -- Docker image repository for the compactor image. Overrides `tempo.image.repository`
        repository: null
        # -- Docker image tag for the compactor image. Overrides `tempo.image.tag`
        tag: null
      # -- The name of the PriorityClass for compactor pods
      priorityClassName: null
      # -- Labels for compactor pods
      podLabels: {}
      # -- Annotations for compactor pods
      podAnnotations: {}
      # -- Additional CLI args for the compactor
      extraArgs: []
      # -- Environment variables to add to the compactor pods
      extraEnv: []
      # -- Environment variables from secrets or configmaps to add to the compactor pods
      extraEnvFrom: []
      # -- Resource requests and limits for the compactor
      resources: {}
      # -- Grace period to allow the compactor to shutdown before it is killed
      terminationGracePeriodSeconds: 30
      # -- Node selector for compactor pods
      nodeSelector: {}
      # -- Tolerations for compactor pods
      tolerations: []
      # -- Extra volumes for compactor pods
      extraVolumeMounts: []
      # -- Extra volumes for compactor deployment
      extraVolumes: []
      config:
        compaction:
          # -- Duration to keep blocks
          block_retention: 48h
          # Duration to keep blocks that have been compacted elsewhere
          compacted_block_retention: 1h
          # -- Blocks in this time window will be compacted together
          compaction_window: 1h
          # -- Amount of data to buffer from input blocks
          v2_in_buffer_bytes: 5242880
          # -- Flush data to backend when buffer is this large
          v2_out_buffer_bytes: 20971520
          # -- Maximum number of traces in a compacted block. WARNING: Deprecated. Use max_block_bytes instead.
          max_compaction_objects: 6000000
          # -- Maximum size of a compacted block in bytes
          max_block_bytes: 107374182400
          # -- Number of tenants to process in parallel during retention
          retention_concurrency: 10
          # -- Number of traces to buffer in memory during compaction
          v2_prefetch_traces_count: 1000
          # -- The maximum amount of time to spend compacting a single tenant before moving to the next
          max_time_per_tenant: 5m
          # -- The time between compaction cycles
          compaction_cycle: 30s
      service:
        # -- Annotations for compactor service
        annotations: {}

    # Configuration for the querier
    querier:
      # -- Number of replicas for the querier
      replicas: 1
      autoscaling:
        # -- Enable autoscaling for the querier
        enabled: false
        # -- Minimum autoscaling replicas for the querier
        minReplicas: 1
        # -- Maximum autoscaling replicas for the querier
        maxReplicas: 3
        # -- Target CPU utilisation percentage for the querier
        targetCPUUtilizationPercentage: 60
        # -- Target memory utilisation percentage for the querier
        targetMemoryUtilizationPercentage:
      image:
        # -- The Docker registry for the querier image. Overrides `tempo.image.registry`
        registry: null
        # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
        pullSecrets: []
        # -- Docker image repository for the querier image. Overrides `tempo.image.repository`
        repository: null
        # -- Docker image tag for the querier image. Overrides `tempo.image.tag`
        tag: null
      # -- The name of the PriorityClass for querier pods
      priorityClassName: null
      # -- Labels for querier pods
      podLabels: {}
      # -- Annotations for querier pods
      podAnnotations: {}
      # -- Additional CLI args for the querier
      extraArgs: []
      # -- Environment variables to add to the querier pods
      extraEnv: []
      # -- Environment variables from secrets or configmaps to add to the querier pods
      extraEnvFrom: []
      # -- Resource requests and limits for the querier
      resources: {}
      # -- Grace period to allow the querier to shutdown before it is killed
      terminationGracePeriodSeconds: 30
      # -- topologySpread for querier pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Defaults to allow skew no more then 1 node per AZ
      topologySpreadConstraints: |
        - maxSkew: 1
          topologyKey: failure-domain.beta.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier") | nindent 6 }}
      # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Hard node and soft zone anti-affinity
      affinity: |
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 10 }}
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 12 }}
                topologyKey: failure-domain.beta.kubernetes.io/zone
      # -- Node selector for querier pods
      nodeSelector: {}
      # -- Tolerations for querier pods
      tolerations: []
      # -- Extra volumes for querier pods
      extraVolumeMounts: []
      # -- Extra volumes for querier deployment
      extraVolumes: []
      config:
        frontend_worker:
          # -- grpc client configuration
          grpc_client_config: {}
        trace_by_id:
          # -- Timeout for trace lookup requests
          query_timeout: 10s
        search:
          # -- A list of external endpoints that the querier will use to offload backend search requests
          external_endpoints: []
          # -- Timeout for search requests
          query_timeout: 30s
          # -- If search_external_endpoints is set then the querier will primarily act as a proxy for whatever serverless backend you have configured. This setting allows the operator to have the querier prefer itself for a configurable number of subqueries.
          prefer_self: 10
          # -- If set to a non-zero value a second request will be issued at the provided duration. Recommended to be set to p99 of external search requests to reduce long tail latency.
          external_hedge_requests_at: 8s
          # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set.
          external_hedge_requests_up_to: 2
        # -- This value controls the overall number of simultaneous subqueries that the querier will service at once. It does not distinguish between the types of queries.
        max_concurrent_queries: 20

      service:
        # -- Annotations for querier service
        annotations: {}
      # -- Adds the appProtocol field to the querier service. This allows querier to work with istio protocol selection.
      appProtocol:
        # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
        grpc: null

    # Configuration for the query-frontend
    queryFrontend:
      query:
        # -- Required for grafana version <7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch
        enabled: false
        image:
          # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`
          registry: null
          # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
          pullSecrets: []
          # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`
          repository: grafana/tempo-query
          # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`
          tag: null
        # -- Resource requests and limits for the query
        resources: {}
        # -- Additional CLI args for tempo-query pods
        extraArgs: []
        # -- Environment variables to add to the tempo-query pods
        extraEnv: []
        # -- Environment variables from secrets or configmaps to add to the tempo-query pods
        extraEnvFrom: []
        # -- Extra volumes for tempo-query pods
        extraVolumeMounts: []
        # -- Extra volumes for tempo-query deployment
        extraVolumes: []
        config: |
          backend: 127.0.0.1:3100
      # -- Number of replicas for the query-frontend
      replicas: 1
      config:
        # -- Number of times to retry a request sent to a querier
        max_retries: 2
        # -- Number of block queries that are tolerated to error before considering the entire query as failed. Numbers greater than 0 make possible for a read to return partial results
        tolerate_failed_blocks: 0
        search:
          # -- The number of concurrent jobs to execute when searching the backend
          concurrent_jobs: 1000
          # -- The target number of bytes for each job to handle when performing a backend search
          target_bytes_per_job: 104857600
        # -- Trace by ID lookup configuration
        trace_by_id:
          # -- The number of shards to split a trace by id query into.
          query_shards: 50
          # -- If set to a non-zero value, a second request will be issued at the provided duration. Recommended to be set to p99 of search requests to reduce long-tail latency.
          hedge_requests_at: 2s
          # -- The maximum number of requests to execute when hedging. Requires hedge_requests_at to be set. Must be greater than 0.
          hedge_requests_up_to: 2
      autoscaling:
        # -- Enable autoscaling for the query-frontend
        enabled: false
        # -- Minimum autoscaling replicas for the query-frontend
        minReplicas: 1
        # -- Maximum autoscaling replicas for the query-frontend
        maxReplicas: 3
        # -- Target CPU utilisation percentage for the query-frontend
        targetCPUUtilizationPercentage: 60
        # -- Target memory utilisation percentage for the query-frontend
        targetMemoryUtilizationPercentage:
      image:
        # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`
        registry: null
        # -- Optional list of imagePullSecrets. Overrides `tempo.image.pullSecrets`
        pullSecrets: []
        # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`
        repository: null
        # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`
        tag: null
      service:
        # -- Annotations for queryFrontend service
        annotations: {}
        # -- Type of service for the queryFrontend
        type: ClusterIP
        # -- If type is LoadBalancer you can assign the IP to the LoadBalancer
        loadBalancerIP: ""
        # -- If type is LoadBalancer limit incoming traffic from IPs.
        loadBalancerSourceRanges: []
      serviceDiscovery:
        # -- Annotations for queryFrontendDiscovery service
        annotations: {}
      # -- The name of the PriorityClass for query-frontend pods
      priorityClassName: null
      # -- Labels for queryFrontend pods
      podLabels: {}
      # -- Annotations for query-frontend pods
      podAnnotations: {}
      # -- Additional CLI args for the query-frontend
      extraArgs: []
      # -- Environment variables to add to the query-frontend pods
      extraEnv: []
      # -- Environment variables from secrets or configmaps to add to the query-frontend pods
      extraEnvFrom: []
      # -- Resource requests and limits for the query-frontend
      resources: {}
      # -- Grace period to allow the query-frontend to shutdown before it is killed
      terminationGracePeriodSeconds: 30
      # -- topologySpread for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Defaults to allow skew no more then 1 node per AZ
      topologySpreadConstraints: |
        - maxSkew: 1
          topologyKey: failure-domain.beta.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 6 }}
      # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
      # @default -- Hard node and soft zone anti-affinity
      affinity: |
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 10 }}
              topologyKey: kubernetes.io/hostname
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 12 }}
                topologyKey: failure-domain.beta.kubernetes.io/zone
      # -- Node selector for query-frontend pods
      nodeSelector: {}
      # -- Tolerations for query-frontend pods
      tolerations: []
      # -- Extra volumes for query-frontend pods
      extraVolumeMounts: []
      # -- Extra volumes for query-frontend deployment
      extraVolumes: []
      # -- Adds the appProtocol field to the queriyFrontend service. This allows queriyFrontend to work with istio protocol selection.
      appProtocol:
        # -- Set the optional grpc service protocol. Ex: "grpc", "http2" or "https"
        grpc: null

    multitenancyEnabled: false

    traces:
      jaeger:
        grpc:
          # -- Enable Tempo to ingest Jaeger GRPC traces
          enabled: false
          # -- Jaeger GRPC receiver config
          receiverConfig: {}
        thriftBinary:
          # -- Enable Tempo to ingest Jaeger Thrift Binary traces
          enabled: false
          # -- Jaeger Thrift Binary receiver config
          receiverConfig: {}
        thriftCompact:
          # -- Enable Tempo to ingest Jaeger Thrift Compact traces
          enabled: false
          # -- Jaeger Thrift Compact receiver config
          receiverConfig: {}
        thriftHttp:
          # -- Enable Tempo to ingest Jaeger Thrift HTTP traces
          enabled: false
          # -- Jaeger Thrift HTTP receiver config
          receiverConfig: {}
      zipkin:
        # -- Enable Tempo to ingest Zipkin traces
        enabled: false
        # -- Zipkin receiver config
        receiverConfig: {}
      otlp:
        http:
          # -- Enable Tempo to ingest Open Telemetry HTTP traces
          enabled: false
          # -- HTTP receiver advanced config
          receiverConfig: {}
        grpc:
          # -- Enable Tempo to ingest Open Telemetry GRPC traces
          enabled: false
          # -- GRPC receiver advanced config
          receiverConfig: {}
      opencensus:
        # -- Enable Tempo to ingest Open Census traces
        enabled: false
        # -- Open Census receiver config
        receiverConfig: {}
      # -- Enable Tempo to ingest traces from Kafka. Reference: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kafkareceiver
      kafka: {}


    # To configure a different storage backend instead of local storage:
    # storage:
    #   trace:
    #     backend: azure
    #     azure:
    #       container_name:
    #       storage_account_name:
    #       storage_account_key:
    storage:
      trace:
        # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage
        backend: s3
      # Settings for the Admin client storage backend and buckets. Only valid is enterprise.enabled is true.
      admin:
        # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/enterprise-traces/latest/config/reference/#admin_client_config
        backend: s3
        bucket:
        endpoints:
        access_key:
        secret_key:
        insecure: false



    # memcached is for all of the Tempo pieces to coordinate with each other.
    # you can use your self memcacherd by set enable: false and host + service
    memcached:
      # -- Specified whether the memcached cachce should be enabled
      enabled: true
      # Number of replicas for memchached
      replicas: 1

    memcachedExporter:
      # -- Specifies whether the Memcached Exporter should be enabled
      enabled: true

    metaMonitoring:
      # ServiceMonitor configuration
      serviceMonitor:
        # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
        enabled: true


    # Rules for the Prometheus Operator
    prometheusRule:
      # -- If enabled, a PrometheusRule resource for Prometheus Operator is created
      enabled: true

    # Configuration for the gateway
    gateway:
      # -- Specifies whether the gateway should be enabled
      enabled: true
      # -- Number of replicas for the gateway
      replicas: 3
      verboseLogging: false
      image:
        # -- The gateway image repository
        repository: nginxinc/nginx-unprivileged
        # -- The gateway image tag
        tag: 1.23-alpine

      # Gateway ingress configuration
      ingress:
        # -- Specifies whether an ingress for the gateway should be created
        enabled: true
        # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
        ingressClassName: nginx
        # -- Annotations for the gateway ingress
        annotations:
          cert-manager.io/cluster-issuer: letsencrypt-production
        # -- Hosts configuration for the gateway ingress
        hosts:
          - host: &host gateway.tempo.${SECRET_PUBLIC_DOMAIN}
            paths:
              - path: /
                # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
                # pathType: Prefix
        # -- TLS configuration for the gateway ingress
        tls:
          - secretName: tempo-gateway-tls
            hosts:
              - *host
